repeats = 5, # for method = "repeatedcv"
## Estimate class probabilities
classProbs = TRUE, # must be True for twoClassSummary
## Evaluate performance using the following function
summaryFunction = twoClassSummary,
## parallelize!
allowParallel = T)
# use grid search (each row is hyperparam)
linearGrid <- expand.grid(C = 2^((1:8) - 4)) # 'Cost' (.125 : 16)
nrow(linearGrid)
model_linear <- train(delay ~ .,
data = test_train,
method = "svmLinear",
trControl = tCtrl,
verbose = FALSE,
## grid search
tuneGrid = linearGrid,
## Specify which metric to optimize
metric = "ROC")
model_linear
# create mini set for fast testing and hyperparameter search
set.seed(123229137)
pct <- .01 # can incrementally increase this
a <- sample(nrow(model_train), pct * nrow(model_train), replace = FALSE)
b <- sample(nrow(model_test), pct * nrow(model_test), replace = FALSE)
test_train <- model_train[a,]
test_test <- model_test[b,]
rm(a,b)
# trainControl controls nuances of train()
tCtrl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 5, # for method = "repeatedcv"
## Estimate class probabilities
classProbs = TRUE, # must be True for twoClassSummary
## Evaluate performance using the following function
summaryFunction = twoClassSummary,
## parallelize!
allowParallel = T)
# use grid search (each row is hyperparam)
linearGrid <- expand.grid(C = 2^((1:8) - 4)) # 'Cost' (.125 : 16)
nrow(linearGrid)
model_linear <- train(delay ~ .,
data = test_train,
method = "svmLinear",
trControl = tCtrl,
verbose = FALSE,
## grid search
tuneGrid = linearGrid,
## Specify which metric to optimize
metric = "ROC")
model_linear
# create mini set for fast testing and hyperparameter search
set.seed(9876)
pct <- .02 # can incrementally increase this
a <- sample(nrow(model_train), pct * nrow(model_train), replace = FALSE)
b <- sample(nrow(model_test), pct * nrow(model_test), replace = FALSE)
test_train <- model_train[a,]
test_test <- model_test[b,]
rm(a,b)
# trainControl controls nuances of train()
tCtrl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 5, # for method = "repeatedcv"
## Estimate class probabilities
classProbs = TRUE, # must be True for twoClassSummary
## Evaluate performance using the following function
summaryFunction = twoClassSummary,
## parallelize!
allowParallel = T)
# use grid search (each row is hyperparam)
linearGrid <- expand.grid(C = 2^((1:8) - 4)) # 'Cost' (.125 : 16)
nrow(linearGrid)
model_linear <- train(delay ~ .,
data = test_train,
method = "svmLinear",
trControl = tCtrl,
verbose = FALSE,
## grid search
tuneGrid = linearGrid,
## Specify which metric to optimize
metric = "ROC")
model_linear
# create mini set for fast testing and hyperparameter search
set.seed(567)
pct <- .02 # can incrementally increase this
a <- sample(nrow(model_train), pct * nrow(model_train), replace = FALSE)
b <- sample(nrow(model_test), pct * nrow(model_test), replace = FALSE)
test_train <- model_train[a,]
test_test <- model_test[b,]
rm(a,b)
# trainControl controls nuances of train()
tCtrl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 5, # for method = "repeatedcv"
## Estimate class probabilities
classProbs = TRUE, # must be True for twoClassSummary
## Evaluate performance using the following function
summaryFunction = twoClassSummary,
## parallelize!
allowParallel = T)
# use grid search (each row is hyperparam)
linearGrid <- expand.grid(C = 2^((1:8) - 4)) # 'Cost' (.125 : 16)
nrow(linearGrid)
model_linear <- train(delay ~ .,
data = test_train,
method = "svmLinear",
trControl = tCtrl,
verbose = FALSE,
## grid search
tuneGrid = linearGrid,
## Specify which metric to optimize
metric = "ROC")
model_linear
# create mini set for fast testing and hyperparameter search
set.seed(11235)
pct <- .02 # can incrementally increase this
a <- sample(nrow(model_train), pct * nrow(model_train), replace = FALSE)
b <- sample(nrow(model_test), pct * nrow(model_test), replace = FALSE)
test_train <- model_train[a,]
test_test <- model_test[b,]
rm(a,b)
# trainControl controls nuances of train()
tCtrl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 5, # for method = "repeatedcv"
## Estimate class probabilities
classProbs = TRUE, # must be True for twoClassSummary
## Evaluate performance using the following function
summaryFunction = twoClassSummary,
## parallelize!
allowParallel = T)
# use grid search (each row is hyperparam)
linearGrid <- expand.grid(C = 2^((1:8) - 4)) # 'Cost' (.125 : 16)
nrow(linearGrid)
model_linear <- train(delay ~ .,
data = test_train,
method = "svmLinear",
trControl = tCtrl,
verbose = FALSE,
## grid search
tuneGrid = linearGrid,
## Specify which metric to optimize
metric = "ROC")
model_linear
a <- sample(nrow(model_train), pct * nrow(model_train), replace = FALSE)
b <- sample(nrow(model_test), pct * nrow(model_test), replace = FALSE)
test_train <- model_train[a,]
test_test <- model_test[b,]
# trainControl controls nuances of train()
tCtrl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 5, # for method = "repeatedcv"
## Estimate class probabilities
classProbs = TRUE, # must be True for twoClassSummary
## Evaluate performance using the following function
summaryFunction = twoClassSummary,
## parallelize!
allowParallel = T)
# use grid search (each row is hyperparam)
linearGrid <- expand.grid(C = 2^((1:8) - 4)) # 'Cost' (.125 : 16)
nrow(linearGrid)
model_linear <- train(delay ~ .,
data = test_train,
method = "svmLinear",
trControl = tCtrl,
verbose = FALSE,
## grid search
tuneGrid = linearGrid,
## Specify which metric to optimize
metric = "ROC")
model_linear
# seed = 9876, 567, 11235, 12322, 9137
set.seed(9137)
a <- sample(nrow(model_train), pct * nrow(model_train), replace = FALSE)
b <- sample(nrow(model_test), pct * nrow(model_test), replace = FALSE)
test_train <- model_train[a,]
test_test <- model_test[b,]
# trainControl controls nuances of train()
tCtrl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 5, # for method = "repeatedcv"
## Estimate class probabilities
classProbs = TRUE, # must be True for twoClassSummary
## Evaluate performance using the following function
summaryFunction = twoClassSummary,
## parallelize!
allowParallel = T)
# use grid search (each row is hyperparam)
linearGrid <- expand.grid(C = 2^((1:8) - 4)) # 'Cost' (.125 : 16)
model_linear <- train(delay ~ .,
data = test_train,
method = "svmLinear",
trControl = tCtrl,
verbose = FALSE,
## grid search
tuneGrid = linearGrid,
## Specify which metric to optimize
metric = "ROC")
model_linear
2^((1:8) - 4
)
2^(2*(1:8) - 4)
2^(2*(1:8) - 8)
2^((.5:4) - 2)
# use grid search (each row is hyperparam)
linearGrid <- expand.grid(C = 2^(2*(1:8) - 8)) # 'Cost' (.125 : 16)
nrow(linearGrid)
model_linear <- train(delay ~ .,
data = test_train,
method = "svmLinear",
trControl = tCtrl,
verbose = FALSE,
## grid search
tuneGrid = linearGrid,
## Specify which metric to optimize
metric = "ROC")
2^(0:10)
2^((0:10)-20
)
2^((0:10)-2)
2^((0:10)-4)
# use grid search (each row is hyperparam)
linearGrid <- expand.grid(C = c(0, 2^(0:10)) # 'Cost' (0:1024)
# use grid search (each row is hyperparam)
linearGrid <- expand.grid(C = c(0, 2^(0:10))) # 'Cost' (0:1024)
nrow(linearGrid)
# use grid search (each row is hyperparam)
linearGrid <- expand.grid(C = c(0, 2^(0:10))) # 'Cost' (0:1024)
nrow(linearGrid)
# trainControl controls nuances of train()
tCtrl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3, # for method = "repeatedcv"
## Estimate class probabilities
classProbs = TRUE, # must be True for twoClassSummary
## Evaluate performance using the following function
summaryFunction = twoClassSummary,
## parallelize!
allowParallel = T)
# seed = 9876, 567, 11235, 12322, 9137
set.seed(9876)
a <- sample(nrow(model_train), pct * nrow(model_train), replace = FALSE)
b <- sample(nrow(model_test), pct * nrow(model_test), replace = FALSE)
test_train <- model_train[a,]
test_test <- model_test[b,]
# stop parallel environment
stopCluster(cl)
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
# trainControl controls nuances of train()
tCtrl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3, # for method = "repeatedcv"
## Estimate class probabilities
classProbs = TRUE, # must be True for twoClassSummary
## Evaluate performance using the following function
summaryFunction = twoClassSummary,
## parallelize!
allowParallel = T)
# use grid search (each row is hyperparam)
linearGrid <- expand.grid(C = c(0, 2^(0:10))) # 'Cost' (0:1024)
# trainControl controls nuances of train()
tCtrl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3, # for method = "repeatedcv"
## Estimate class probabilities
classProbs = TRUE, # must be True for twoClassSummary
## Evaluate performance using the following function
summaryFunction = twoClassSummary,
## parallelize!
allowParallel = T)
# use grid search (each row is hyperparam)
linearGrid <- expand.grid(C = c(0, 2^(0:10))) # 'Cost' (0:1024)
nrow(linearGrid)
model_linear <- train(delay ~ .,
data = test_train,
method = "svmLinear",
trControl = tCtrl,
verbose = FALSE,
## grid search
tuneGrid = linearGrid,
## Specify which metric to optimize
metric = "ROC")
pct <- .02 # can incrementally increase this
# seed = 9876, 567, 11235, 12322, 9137
set.seed(9876)
a <- sample(nrow(model_train), pct * nrow(model_train), replace = FALSE)
b <- sample(nrow(model_test), pct * nrow(model_test), replace = FALSE)
test_train <- model_train[a,]
test_test <- model_test[b,]
rm(a,b)
# stop parallel environment
stopCluster(cl)
#install.packages("e1071")
#install.packages("kernlab")
#install.packages("pROC")
#install.packages("microbenchmark")
#install.packages("doParallel")
library(tidyverse)
library(skimr)
library(openxlsx)
library(lubridate)
library(doParallel)
library(caret)
library(e1071)
#library(kernlab) # ksvm
library(pROC)
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
# seed = 9876, 567, 11235, 12322, 9137
set.seed(9876)
a <- sample(nrow(model_train), pct * nrow(model_train), replace = FALSE)
b <- sample(nrow(model_test), pct * nrow(model_test), replace = FALSE)
test_train <- model_train[a,]
test_test <- model_test[b,]
rm(a,b)
# trainControl controls nuances of train()
tCtrl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3, # for method = "repeatedcv"
## Estimate class probabilities
classProbs = TRUE, # must be True for twoClassSummary
## Evaluate performance using the following function
summaryFunction = twoClassSummary,
## parallelize!
allowParallel = T)
### linear kernel
# use grid search (each row is hyperparam)
linearGrid <- expand.grid(C = c(0, 2^(0:10))) # 'Cost' (0:1024)
nrow(linearGrid)
model_linear <- train(delay ~ .,
data = test_train,
method = "svmLinear",
trControl = tCtrl,
verbose = FALSE,
## grid search
tuneGrid = linearGrid,
## Specify which metric to optimize
metric = "ROC")
model_linear
### Notes:
# seed = 9876, 567, 11235, 12322, 9137
# C =     ,     ,     ,     ,     .
# only the most minor differences between costs
# plot grid results
trellis.par.set(caretTheme())
plot(model_linear, metric = "ROC") # "ROC" is parameter (alternative, "Kappa")
### radial kernel
# use grid search (each row is hyperparam)
radialGrid <-  expand.grid(sigma = 10 ^((1:6) - 4), #'Sigma' (.001 : 100)
C = c(0, 2^(0:10))) # 'Cost' (0:1024)
nrow(radialGrid)
model_radial <- train(delay ~ .,
data = test_train,
method = "svmRadial",
trControl = tCtrl,
verbose = FALSE,
## grid search
tuneGrid = radialGrid,
## Specify which metric to optimize
metric = "ROC")
model_radial
### Notes:
# (seed 9876)
# sigma = .01
# C = 8
# plot grid results
trellis.par.set(caretTheme())
plot(model_radial, metric = "ROC") # "ROC" is parameter (alternative, "Kappa")
### polynomial kernel
# use grid search (each row is hyperparam)
polyGrid <-  expand.grid(degree = seq(1, 3), # 'Polynomial Degree' (^1, ^2, ^3)
scale = 10 ^((1:6) - 4), # 'Scale' (.001 : 100)
C = c(0, 2^(0:10))) # 'Cost' (0:1024)
nrow(polyGrid)
model_poly <- train(delay ~ .,
data = test_train,
method = "svmPoly",
trControl = tCtrl,
verbose = FALSE,
## grid search
tuneGrid = polyGrid,
## Specify which metric to optimize
metric = "ROC")
model_poly
### Notes:
# (seed 9876)
# degree =
# scale =
# C =
# plot grid results
trellis.par.set(caretTheme())
plot(model_poly, metric = "ROC") # "ROC" is parameter (alternative, "Kappa")
# stop parallel environment
stopCluster(cl)
#-- compare model(s) ------------------------------------------------------------------------------
resamps <- resamples(list(linear = model_linear,
radial = model_radial,
poly = model_poly))
resamps
summary(resamps)
# plot
trellis.par.set(caretTheme())
dotplot(resamps, metric = "ROC")
difValues <- diff(resamps)
difValues
summary(difValues)
# plot differences
trellis.par.set(caretTheme())
bwplot(difValues, layout = c(3, 1))
10 ^((1:6) - 4
)
10^(-4:4)
1e+04
10 ^(-5:1)
10 ^(-5:0)
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
# trainControl controls nuances of train()
tCtrl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3, # for method = "repeatedcv"
## Estimate class probabilities
classProbs = TRUE, # must be True for twoClassSummary
## Evaluate performance using the following function
summaryFunction = twoClassSummary,
## parallelize!
allowParallel = T)
# use grid search (each row is hyperparam)
radialGrid <-  expand.grid(sigma = 10 ^(-5:0), #'Sigma' (.001 : 100)
C = c(0, 2^(0:10))) # 'Cost' (0:1024)
nrow(radialGrid)
model_radial <- train(delay ~ .,
data = test_train,
method = "svmRadial",
trControl = tCtrl,
verbose = FALSE,
## grid search
tuneGrid = radialGrid,
## Specify which metric to optimize
metric = "ROC")
model_radial
resamps <- resamples(list(linear = model_linear,
radial = model_radial,
poly = model_poly))
resamps
# stop parallel environment
stopCluster(cl)
resamps <- resamples(list(linear = model_linear,
radial = model_radial,
poly = model_poly))
# plot
trellis.par.set(caretTheme())
dotplot(resamps, metric = "ROC")
difValues <- diff(resamps)
summary(difValues)
# plot differences
trellis.par.set(caretTheme())
bwplot(difValues, layout = c(3, 1))
# plot grid results
trellis.par.set(caretTheme())
plot(model_radial, metric = "ROC") # "ROC" is parameter (alternative, "Kappa")
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
### parallelize
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
2^(0:10)
10 ^(-5:0)
1e-05
.00001
c(.0015, 10^(-5:1))
# trainControl controls nuances of train()
tCtrl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3, # for method = "repeatedcv"
## Estimate class probabilities
classProbs = TRUE, # must be True for twoClassSummary
## Evaluate performance using the following function
summaryFunction = twoClassSummary,
## parallelize!
allowParallel = T)
# create slightly larger set for fast testing and hyperparameter search
set.seed(12322)
pct <- .3 # can incrementally increase this
a <- sample(nrow(model_train), pct * nrow(model_train), replace = FALSE)
b <- sample(nrow(model_test), pct * nrow(model_test), replace = FALSE)
test_train <- model_train[a,]
test_test <- model_test[b,]
rm(a,b)
### parallelize
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
### linear kernel
# use grid search (each row is hyperparam)
linearGrid <- expand.grid(C = c(75, 100, 125, 150, 175, 200)) # 'Cost'
nrow(linearGrid)
model_linear <- train(delay ~ .,
data = test_train,
method = "svmLinear",
trControl = tCtrl,
verbose = FALSE,
## grid search
tuneGrid = linearGrid,
## Specify which metric to optimize
metric = "ROC")
